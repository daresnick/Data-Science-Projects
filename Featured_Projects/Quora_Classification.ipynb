{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quora Insincere Questions Classification\n",
    "### Detect toxic content to improve online conversations\n",
    "### Binary Text Classification\n",
    "#### Damon Resnick, 3/7/2019\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:53:56.677745Z",
     "start_time": "2019-03-11T23:53:48.680747Z"
    }
   },
   "outputs": [],
   "source": [
    "# import the pandas library needed for loading the data\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', 1000) # This allows to us see the entire text string in a table using pandas\n",
    "\n",
    "# numpy was used in a few cases\n",
    "import numpy as np\n",
    "\n",
    "# SMOTE may be used in one instance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Use ignore warnings in the final running of the notebook to clean it up in one place.\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:54:00.585333Z",
     "start_time": "2019-03-11T23:53:56.680725Z"
    }
   },
   "outputs": [],
   "source": [
    "# load in the data as a data frame using pandas\n",
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:54:00.619300Z",
     "start_time": "2019-03-11T23:54:00.587319Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province as a nation in the 1960s?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you encourage people to adopt and not shop?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity affect space geometry?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg hemispheres?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid  \\\n",
       "0  00002165364db923c7e6   \n",
       "1  000032939017120e6e44   \n",
       "2  0000412ca6e4628ce2cf   \n",
       "3  000042bf85aa498cd78e   \n",
       "\n",
       "                                                                       question_text  \\\n",
       "0           How did Quebec nationalists see their province as a nation in the 1960s?   \n",
       "1  Do you have an adopted dog, how would you encourage people to adopt and not shop?   \n",
       "2                Why does velocity affect time? Does velocity affect space geometry?   \n",
       "3                          How did Otto von Guericke used the Magdeburg hemispheres?   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:54:00.642284Z",
     "start_time": "2019-03-11T23:54:00.623295Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is it crazy if I wash or wipe my groceries off? Germs are everywhere.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['question_text'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:54:00.718237Z",
     "start_time": "2019-03-11T23:54:00.648281Z"
    }
   },
   "outputs": [],
   "source": [
    "data.rename(columns = {'$q':'text'}, inplace=True)\n",
    "data.columns = ['id', 'text', 'flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:54:00.730230Z",
     "start_time": "2019-03-11T23:54:00.720236Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province as a nation in the 1960s?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you encourage people to adopt and not shop?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity affect space geometry?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg hemispheres?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id  \\\n",
       "0  00002165364db923c7e6   \n",
       "1  000032939017120e6e44   \n",
       "2  0000412ca6e4628ce2cf   \n",
       "3  000042bf85aa498cd78e   \n",
       "\n",
       "                                                                                text  \\\n",
       "0           How did Quebec nationalists see their province as a nation in the 1960s?   \n",
       "1  Do you have an adopted dog, how would you encourage people to adopt and not shop?   \n",
       "2                Why does velocity affect time? Does velocity affect space geometry?   \n",
       "3                          How did Otto von Guericke used the Magdeburg hemispheres?   \n",
       "\n",
       "   flag  \n",
       "0     0  \n",
       "1     0  \n",
       "2     0  \n",
       "3     0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:54:01.128982Z",
     "start_time": "2019-03-11T23:54:00.733228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1306122 entries, 0 to 1306121\n",
      "Data columns (total 3 columns):\n",
      "id      1306122 non-null object\n",
      "text    1306122 non-null object\n",
      "flag    1306122 non-null int64\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 29.9+ MB\n",
      "None\n",
      "Types of flags: [0 1]\n"
     ]
    }
   ],
   "source": [
    "print(data.info())\n",
    "print('Types of flags:', data['flag'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep the data for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:54:01.195942Z",
     "start_time": "2019-03-11T23:54:01.130983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of rows 1306122\n",
      "Number of 1s in flag 80810\n",
      "Number of 0s in flag 1225312\n"
     ]
    }
   ],
   "source": [
    "# The data appears to be severely unbalanced: many more 0 flags than 1 flags!\n",
    "print('Total Number of rows', data['flag'].count())\n",
    "print('Number of 1s in flag', data['flag'].sum())\n",
    "print('Number of 0s in flag', data['flag'].count() - data['flag'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the data is unbalanced. However it is not clear if this is a problem or not. It would make sense that modeling the data without addressing this imbalance, biases the model towards correctly predicting 0s instead of 1s, but if the data in the future is like that as well then this might actually make sense. **Without correcting for imbalance with a simple model only 5.8% of 0s are incorrectly predicted while about 40% of the 1s are incorrectly predicted.** Whether there is a need to correct for this is debatable. However it is possible that correcting for the imbalance will also increase the over all accuracy and increase other prediction metrics like recall and precision. But if it is possible to increase the accuracy of predicting the 1s without decreasing the accuracy of predicting the 0s then that method will be a better overall method.\n",
    "  \n",
    "In order to try and correct for the imbalance we will try a few different methods and test them on a held out portion of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rebalance the data set: Two options\n",
    "    - Over-sample the minority class, the 1s, such that there are as many as the 0s\n",
    "    - Under-sample the majority class, the 0s, such that there are as many as the 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a hold out set:\n",
    "In order to determine which method will give a better result we will hold out a significant portion of the training data to test the different methods in order to have a test set of data that is the same for each method that has remained unaffected. We have chosen the hold out set to be 375,811 rows because the already held out data, the test data, we wish to predict the classes for has 375,811 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:54:01.820572Z",
     "start_time": "2019-03-11T23:54:01.197942Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data will be split up to make a hold out set. The test set is 375,811 rows\n",
    "# so we will make the hold out set the same size.\n",
    "train = data.sample(n=375811, replace=False, random_state=42)\n",
    "hold = data.drop(train.index)\n",
    "\n",
    "train = train.reset_index(drop=True) # reset the index of the train2 set\n",
    "hold = hold.reset_index(drop=True) # reset the index of the hold set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over-sample the minority class:\n",
    "This adds many 1s by adding a random sample of the 1s rows to the data. This method may be a bit of a problem as it will then tend to bias the model towards the minority class even though the class would still be minority in a real word data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:54:02.599078Z",
     "start_time": "2019-03-11T23:54:01.822557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of majority class: 352922\n",
      "Size of minority class: 22889\n",
      "Difference in classes: 330033\n",
      "New size of majority class: 352922\n",
      "New size of minority class: 352922\n"
     ]
    }
   ],
   "source": [
    "# First we need to know how many majority and minority rows there are.\n",
    "num_maj = train['flag'].count() - train['flag'].sum()\n",
    "num_min = train['flag'].sum()\n",
    "differ = (num_maj - num_min)\n",
    "\n",
    "print('Size of majority class:', num_maj)\n",
    "print('Size of minority class:', num_min)\n",
    "print('Difference in classes:', differ)\n",
    "\n",
    "# We will then make the size of the class balanced by adding the difference to the minority class\n",
    "balanced_os = pd.concat([train, \n",
    "                         train[train['flag'] == 1].sample(n=differ, replace=True, random_state=42)], \n",
    "                        ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True) \n",
    "\n",
    "X_os = balanced_os['text']\n",
    "y_os = balanced_os['flag'] # Create a series to store the target: y\n",
    "\n",
    "\n",
    "print('New size of majority class:', y_os.count() - y_os.sum())\n",
    "print('New size of minority class:', y_os.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over sample using SMOTE  \n",
    "  \n",
    "-  SMOTE or the Synthetic Minority Over-sampling Technique is another method to over sample. Instead of simply replicating the minority class rows SMOTE uses those existing minority instances to create new ones. It uses a k-nearest neighbors technique to do this.  \n",
    "  \n",
    "SMOTE requires continuous samples so this will have to be done after the vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under-sample the majority class:\n",
    "Since there are only 22889 1s, we randomly sample only 22889 of the 0s then create a new data set by adding those samples to the 1s rows. This creates a training data set that is only 45778 rows which is roughly 3.5% the size of the original data set. We are throwing away a lot of data so this method may need to be modified so that accuracy is not reduced. It may be useful to use an ensemble approach here to make sure we are using all the data. For the ensemble approach the 0s and 1s rows will be separated. The 0s will be split up into roughly 15 different data sets then combined with the 1s each. This will create 15 different data sets where each has the same 1s rows but different 0s rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:54:02.732000Z",
     "start_time": "2019-03-11T23:54:02.605074Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New size of majority class: 22889\n",
      "New size of minority class: 22889\n"
     ]
    }
   ],
   "source": [
    "# Create Undersampled set\n",
    "balanced_us = pd.concat([train[train['flag'] == 0].sample(n=num_min, replace=True, random_state=42), \n",
    "                         train[train['flag'] == 1]], ignore_index=True)\n",
    "\n",
    "X_us = balanced_us['text']\n",
    "y_us = balanced_us['flag'] # Create a series to store the target: y\n",
    "\n",
    "print('New size of majority class:', y_us.count() - y_us.sum())\n",
    "print('New size of minority class:', y_us.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:54:03.387594Z",
     "start_time": "2019-03-11T23:54:02.733998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data sets created: 15\n"
     ]
    }
   ],
   "source": [
    "# This code cell will create 14 dfs that are each almost perfectly balanced\n",
    "\n",
    "# I want to split the majority class into 14 separate dfs, all of equal size to the minority data set.\n",
    "# Then create 14 new balanced data sets. Using these sets I would then fit each to the model.\n",
    "# I will then take each fit and predict_prob averaging the predicted probs for each model.\n",
    "# Then round to get the prediction.\n",
    "\n",
    "# First I need to split up the majority class into 14 separate dfs.\n",
    "\n",
    "# randomly shuffle data set and reset the index\n",
    "df_resample = train[train['flag'] == 0].sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Number of data sets needed to make it balanced.\n",
    "num_splits = (num_maj/num_min).round(0).astype(int)\n",
    "\n",
    "num = [int(n.round(0)) for n in np.arange(0, len(df_resample), len(df_resample)/num_splits)] # locations of splits\n",
    "\n",
    "df_all_maj = []\n",
    "\n",
    "# Make a list of the 13 dfs of the majority class by splitting the df 13 times\n",
    "for i in range(len(num)):\n",
    "    if i < len(num)-1:\n",
    "        df_all_maj.append(df_resample.loc[num[i]:num[i+1]-1,:])\n",
    "    else:\n",
    "        df_all_maj.append(df_resample.loc[num[i]:,:])\n",
    "\n",
    "\n",
    "df_min = train[train['flag'] == 1] # minority df\n",
    "\n",
    "# Now take each df in the list and combine it with the minority df, df_min\n",
    "df_all_us = [pd.concat([df, df_min]).sample(frac=1, random_state=42).reset_index(drop=True) for df in df_all_maj]\n",
    "\n",
    "print('Number of data sets created:', len(df_all_us))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:54:03.506523Z",
     "start_time": "2019-03-11T23:54:03.390593Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make an initial train test split to test vectorizers\n",
    "# This is used with the modeling below to run basic comparisons\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train['text']\n",
    "y = train['flag']\n",
    "\n",
    "# Create training and test sets in order to tune and compare models\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use a method to vectorize the text into a form that a classification model can use to make prediction with.\n",
    "  \n",
    "There are basically two vectorizers: Count and TF-IDF\n",
    "\n",
    "Count Vectorizer: The most straightforward one, it counts the number of times a token shows up in the document and uses this value as its weight.  \n",
    "\n",
    "TF-IDF Vectorizer: TF-IDF stands for “term frequency-inverse document frequency”, meaning the weight assigned to each token not only depends on its frequency in a document but also how recurrent that term is in the entire corpora. More on that here.\n",
    "  \n",
    "https://towardsdatascience.com/hacking-scikit-learns-vectorizers-9ef26a7170af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:54:12.848766Z",
     "start_time": "2019-03-11T23:54:03.507520Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '0000', '00000000000000000021e800', '00000001', '000000199', '000009808', '000009820', '000009920', '000125']\n"
     ]
    }
   ],
   "source": [
    "# Try CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "#count_vectorizer = CountVectorizer(stop_words='english')\n",
    "count_vectorizer = CountVectorizer() # Slightly better results without stop words\n",
    "\n",
    "# Fit and Transform the training data using only the 'text' column values: count_train \n",
    "count_train = count_vectorizer.fit_transform(X_train.values)\n",
    "\n",
    "# Transform the test data using only the 'text' column values: count_test \n",
    "count_test = count_vectorizer.transform(X_test.values)\n",
    "\n",
    "# Print the first 10 features of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:54:22.536801Z",
     "start_time": "2019-03-11T23:54:12.851767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '0000', '00000000000000000021e800', '00000001', '000000199', '000009808', '000009820', '000009920', '000125']\n"
     ]
    }
   ],
   "source": [
    "# Try TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "#tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.7)\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.7) # Slightly better results without stop words\n",
    "\n",
    "# Fit and Transform the training data: tfidf_train \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train.values)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test.values)\n",
    "\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "#print(tfidf_train.A[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model the Data with Naive Bayes Classifier\n",
    "  \n",
    "A Naive Bayes Classifier is used here to try different balancing methods as it is efficient and fast and make sense for the type of text and problem we have. Other models may be chosen for the final fit to maximize the prediction scores, but we use NB here to determine which method of balancing works best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will see if one Vectorizer works better than the other by fitting them with the balanced over-sampled data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:54:23.708081Z",
     "start_time": "2019-03-11T23:54:22.538800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.9209052326277557\n",
      "F1: 0.4826385867200417\n",
      "Prec: 0.4032867946480512\n",
      "Rec: 0.6008667388949079\n",
      "[[66445  4103]\n",
      " [ 1842  2773]]\n"
     ]
    }
   ],
   "source": [
    "# Use a Naive Bayes method to model the CountVectorizer data\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier using alpha to tune\n",
    "nb_classifier = MultinomialNB(alpha=0.0000000001) # alpha = 0 seems to be the best for CountVectorizer\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(count_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print('Acc:', score)\n",
    "print('F1:', metrics.f1_score(y_test, pred))\n",
    "print('Prec:', metrics.precision_score(y_test, pred))\n",
    "print('Rec:', metrics.recall_score(y_test, pred))\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=[0,1])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:54:24.793412Z",
     "start_time": "2019-03-11T23:54:23.710079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9447999680693958\n",
      "F1: 0.4006933410371226\n",
      "Prec: 0.6009532062391681\n",
      "Rec: 0.30054171180931744\n",
      "[[69627   921]\n",
      " [ 3228  1387]]\n"
     ]
    }
   ],
   "source": [
    "# Use a Naive Bayes method to model the TfidfVectorizer data\n",
    "\n",
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier tune with different alphas\n",
    "nb_classifier_t = MultinomialNB(alpha=0.0000000001) # alpha = 0 seems to give the best results\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier_t.fit(tfidf_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred_t = nb_classifier_t.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score_t = metrics.accuracy_score(y_test, pred_t)\n",
    "print(score_t)\n",
    "print('F1:', metrics.f1_score(y_test, pred_t))\n",
    "print('Prec:', metrics.precision_score(y_test, pred_t))\n",
    "print('Rec:', metrics.recall_score(y_test, pred_t))\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm_t = metrics.confusion_matrix(y_test, pred_t, labels=[0,1])\n",
    "print(cm_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the two vectorizers give very similar base accuracy scores. Tfidf does slightly better at accuracy but much worse at other metrics. In fact Tfidf does predicts nearly 70% of the 1s as 0s, compared to the Count model of 40%. The difference in the confusion matrix shows that mix of TPs, TNs, FPs, and FNs are much different. Which Vectorizer we use should be determined by the problem we are trying to answer.\n",
    "  \n",
    "The type of vectorizer seems to make some difference here. We will use the **Count** vectorizer to start and later come back and make sure it was the best choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automate the vectorization and modeling\n",
    "  \n",
    "Here we automate the vectorization and modeling into one function to make it easier to compare the models on different methods of balancing the data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:54:24.813401Z",
     "start_time": "2019-03-11T23:54:24.800409Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make a function to perform all the data transformation and the modeling\n",
    "# The function should take as input:\n",
    "# X_train, y_train, X_test, y_test, and the tuned model to be fit and then predicted\n",
    "\n",
    "def automate_tf1(X_train, y_train, X_test, y_test, model):\n",
    "    \n",
    "    ''' This function takes X_train, y_train, X_test, y_test, and the a tuned classification model then fits it and outputs\n",
    "    scores and a confusion matrix as well as the probabilities of the classification model.\n",
    "    '''\n",
    "\n",
    "    count_vectorizer = CountVectorizer() # Slightly better without stop words\n",
    "\n",
    "    # Fit and Transform the training data: X_train_t \n",
    "    X_train_t = count_vectorizer.fit_transform(X_train.values)\n",
    "\n",
    "    # Transform the test data: X_test_t \n",
    "    X_test_t = count_vectorizer.transform(X_test.values)\n",
    "\n",
    "    # Fit the classifier to the training data\n",
    "    model.fit(X_train_t, y_train)\n",
    "\n",
    "    # Create the predicted tags: pred\n",
    "    pred = model.predict(X_test_t)\n",
    "    pred_probs = model.predict_proba(X_test_t)\n",
    "\n",
    "    # Calculate the accuracy score:\n",
    "    print('Acc:', metrics.accuracy_score(y_test, pred))\n",
    "    print('F1:', metrics.f1_score(y_test, pred))\n",
    "    print('Prec:', metrics.precision_score(y_test, pred))\n",
    "    print('Rec:', metrics.recall_score(y_test, pred))\n",
    "\n",
    "    # Calculate the confusion matrix: cm\n",
    "    print(metrics.confusion_matrix(y_test, pred, labels=[0,1]))\n",
    "\n",
    "    return (pred_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare different models using the Hold out set\n",
    "Now we will compare the different methods and models to the held out data instead of using test data from the modified train2 data.  \n",
    "  \n",
    "The key here is that the test data is the 1000 rows held out before any of the data was changed to make it more balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:55:21.785145Z",
     "start_time": "2019-03-11T23:54:37.574372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.8601489179424945\n",
      "F1: 0.3895996659582354\n",
      "Prec: 0.26748740546042543\n",
      "Rec: 0.7168557172700748\n",
      "[[758685 113705]\n",
      " [ 16400  41521]]\n"
     ]
    }
   ],
   "source": [
    "# Use the automate function to compare different models to the hold out set\n",
    "\n",
    "# Train\n",
    "# Xtra = train2['text']\n",
    "# ytra = train2['PreventiveFlag']\n",
    "\n",
    "Xtra = X_os\n",
    "ytra = y_os\n",
    "\n",
    "# Test\n",
    "Xtea = hold['text']\n",
    "ytea = hold['flag']\n",
    "\n",
    "# Tuned Model\n",
    "model = MultinomialNB(alpha=0.0000000001)\n",
    "#model = XGBClassifier(max_depth=3, learning_rate=0.9, n_estimators=300, min_child_weight=0)\n",
    "\n",
    "probs = automate_tf1(Xtra, ytra, Xtea, ytea, model = model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a function to take multiple data sets and average the prediction probabilities in order to create an ensemble of results.  \n",
    "  \n",
    "This function will make it so we can compare different ensembles of each method in an attempt to find a combination that improves the overall model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T00:12:49.242375Z",
     "start_time": "2019-03-08T00:12:49.220389Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make a function to perform all the data transformation and the modeling\n",
    "# The function should take as input:\n",
    "# X_train, y_train, X_test, y_test, and the tuned modeling to be fit and then predicted\n",
    "\n",
    "def automate_tf2(X_train, y_train, X_test, y_test, model, is_SMOTE = False, tfidf_vect = False):\n",
    "    ''' This function takes X_train, y_train, X_test, y_test, and the a tuned classification model then fits it and outputs\n",
    "    scores and a confusion matrix as well as the probabilities of the classification model.\n",
    "    '''\n",
    "    \n",
    "    if tfidf_vect == True:\n",
    "        vectorizer = TfidfVectorizer(max_df=0.7)\n",
    "    else:\n",
    "        vectorizer = CountVectorizer()\n",
    "\n",
    "    # Fit and Transform the training data: X_train_t \n",
    "    X_train_t = vectorizer.fit_transform(X_train.values)\n",
    "\n",
    "    # Transform the test data: X_test_t \n",
    "    X_test_t = vectorizer.transform(X_test.values)\n",
    "        \n",
    "    if is_SMOTE == True:\n",
    "        sm = SMOTE(random_state=12, ratio = 1.0)\n",
    "        X_train_t, y_train = sm.fit_sample(X_train_t, y_train)\n",
    "    \n",
    "    # Fit the classifier to the training data\n",
    "    model.fit(X_train_t, y_train)\n",
    "    \n",
    "    pred_probs = model.predict_proba(X_test_t)\n",
    "\n",
    "    return (pred_probs)\n",
    "\n",
    "\n",
    "# Automate function to acount for ensembling comparisons\n",
    "\n",
    "def auto_ensemble(list_dfs, X_test, y_test, model, is_SMOTE = False):\n",
    "    ''' This function automates the ensemble process. It takes a list of dfs, models the data, and averages the results\n",
    "    of each data set then takes the average results and computes the scores and confusion matrix of the ensembled results. \n",
    "    inputs:\n",
    "    list_dfs is a list of data frames with both x and y columns.\n",
    "    X_test, y_test: are pandas series presumably from the hold out set.\n",
    "    model: model to be used in order of the list of dfs\n",
    "    '''\n",
    "    \n",
    "    pred_probs_tot = 0\n",
    "        \n",
    "    for df in list_dfs:\n",
    "        X_train = df['text']\n",
    "        y_train = df['flag']\n",
    "        pred_prob0 = automate_tf2(X_train, y_train, X_test, y_test, model, is_SMOTE)\n",
    "        pred_probs_tot = pred_probs_tot + np.array(pred_prob0)\n",
    "    \n",
    "    pred_probs_ave = pred_probs_tot[:,1]/len(list_dfs)\n",
    "    \n",
    "    pred_ave_int = pred_probs_ave.round(0).astype(int)\n",
    "\n",
    "    # Calculate the accuracy score:\n",
    "    print('Acc:', metrics.accuracy_score(y_test, pred_ave_int))\n",
    "    print('F1:', metrics.f1_score(y_test, pred_ave_int))\n",
    "    print('Prec:', metrics.precision_score(y_test, pred_ave_int))\n",
    "    print('Rec:', metrics.recall_score(y_test, pred_ave_int))\n",
    "\n",
    "    # Calculate the confusion matrix: cm\n",
    "    print(metrics.confusion_matrix(y_test, pred_ave_int, labels=[0,1]))\n",
    "    \n",
    "    return (pred_probs_ave, pred_ave_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make some different lists of dfs so it is easy to see what combinations are being tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T03:20:05.602879Z",
     "start_time": "2019-03-07T03:20:05.582891Z"
    }
   },
   "outputs": [],
   "source": [
    "# This will put a lot of dfs into memory. To save memory you may want to do this one at a time\n",
    "\n",
    "# Make a list of dfs that have all under-sample, over-sample, and the original data set\n",
    "df_both_os = pd.concat([X_os, y_os], axis=1) # need to pass a list of dfs to the function\n",
    "df_new_list1 = df_all_us.copy()\n",
    "df_new_list1.extend([df_both_os])\n",
    "df_new_list1.extend([train])\n",
    "\n",
    "# Make a list of dfs that have one under-sample, over-sample, and the original data set\n",
    "df_both_us = pd.concat([X_us, y_us], axis=1) # need to pass a list of dfs to the function\n",
    "df_new_list2 = [df_both_us].copy()\n",
    "df_new_list2.extend([df_both_os])\n",
    "df_new_list2.extend([train])\n",
    "\n",
    "# Make a list of dfs that have over-sample and the original data set\n",
    "df_new_list3 = [df_both_os].copy()\n",
    "df_new_list3.extend([train])\n",
    "\n",
    "# Make a list of dfs that have one under-sample and the original data set\n",
    "df_new_list4 = [df_both_us].copy()\n",
    "df_new_list4.extend([train])\n",
    "\n",
    "# Make a list of dfs that have all under-sample and the over sample\n",
    "df_new_list5 = df_all_us.copy()\n",
    "df_new_list5.extend([df_both_os])\n",
    "\n",
    "# Make a list of dfs that have all under-sample and the over sample\n",
    "df_new_list6 = [df_both_us].copy()\n",
    "df_new_list6.extend([df_both_os])\n",
    "\n",
    "# Make a list of dfs that have all under-sample, and the original data set\n",
    "df_new_list7 = df_all_us.copy()\n",
    "df_new_list7.extend([train])\n",
    "\n",
    "# Test set is the hold out set\n",
    "Xtea = hold['text']\n",
    "ytea = hold['flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T00:13:34.471816Z",
     "start_time": "2019-03-08T00:13:02.336647Z"
    }
   },
   "outputs": [],
   "source": [
    "# Original data using NB model\n",
    "\n",
    "df_list = [train]\n",
    "\n",
    "# Test set is the hold out set\n",
    "Xtea = hold['text']\n",
    "ytea = hold['flag']\n",
    "\n",
    "# Tuned Model\n",
    "model = MultinomialNB(alpha=0.0000000001)\n",
    "\n",
    "print('Original Train data')\n",
    "\n",
    "probs = auto_ensemble(df_list, Xtea, ytea, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T23:07:02.023389Z",
     "start_time": "2019-03-07T22:58:46.638909Z"
    }
   },
   "outputs": [],
   "source": [
    "# Original data using NB model using SMOTE\n",
    "\n",
    "df_list = [train]\n",
    "\n",
    "# Tuned Model\n",
    "model = MultinomialNB(alpha=0.0000000001)\n",
    "\n",
    "print('Original Train data')\n",
    "\n",
    "probs = auto_ensemble(df_list, Xtea, ytea, model = model, is_SMOTE = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T23:10:04.495761Z",
     "start_time": "2019-03-07T23:08:40.816848Z"
    }
   },
   "outputs": [],
   "source": [
    "# Original data using XGBoost model\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "df_list = [train]\n",
    "\n",
    "# Tuned Model\n",
    "model = XGBClassifier(max_depth=3, learning_rate=0.9, n_estimators=50, min_child_weight=0)\n",
    "\n",
    "print('Original data with XGBoost model')\n",
    "\n",
    "probs = auto_ensemble(df_list, Xtea, ytea, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T23:44:35.307155Z",
     "start_time": "2019-03-07T23:26:35.022176Z"
    }
   },
   "outputs": [],
   "source": [
    "# Original data using NB model using SMOTE and XGB\n",
    "\n",
    "df_list = [train]\n",
    "\n",
    "# Tuned Model\n",
    "model = XGBClassifier(max_depth=3, learning_rate=0.9, n_estimators=300, min_child_weight=0)\n",
    "\n",
    "print('Original Train data with SMOTE and XGB')\n",
    "\n",
    "probs = auto_ensemble(df_list, Xtea, ytea, model = model, is_SMOTE = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The XGB classifier does a slightly better job at fitting the minority class, but not quite as good with the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T03:20:54.107176Z",
     "start_time": "2019-03-07T03:20:53.729410Z"
    }
   },
   "outputs": [],
   "source": [
    "# Over-sampled model\n",
    "\n",
    "df_list = [df_both_os]\n",
    "\n",
    "# Tuned Model\n",
    "model = MultinomialNB(alpha=0.0000000001)\n",
    "\n",
    "print('Over-sampled model')\n",
    "\n",
    "probs = auto_ensemble(df_list, Xtea, ytea, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T03:21:11.459348Z",
     "start_time": "2019-03-07T03:20:54.112174Z"
    }
   },
   "outputs": [],
   "source": [
    "# Over-sampled model with XGB\n",
    "\n",
    "df_list = [df_both_os]\n",
    "\n",
    "# Tuned Model\n",
    "model = XGBClassifier(max_depth=3, learning_rate=0.9, n_estimators=300, min_child_weight=0)\n",
    "\n",
    "print('Over-sampled model with XGB')\n",
    "\n",
    "probs = auto_ensemble(df_list, Xtea, ytea, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T03:21:11.540297Z",
     "start_time": "2019-03-07T03:21:11.461350Z"
    }
   },
   "outputs": [],
   "source": [
    "# Under-sampled model\n",
    "\n",
    "df_list = [df_both_us]\n",
    "\n",
    "# Tuned Model\n",
    "model = MultinomialNB(alpha=0.0000000001)\n",
    "\n",
    "print('Under-sampled model')\n",
    "\n",
    "probs = auto_ensemble(df_list, Xtea, ytea, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T03:21:12.501743Z",
     "start_time": "2019-03-07T03:21:11.543299Z"
    }
   },
   "outputs": [],
   "source": [
    "# Under-sampled model using XGB as the accuracy was very low\n",
    "\n",
    "df_list = [df_both_us]\n",
    "\n",
    "# Tuned Model\n",
    "model = XGBClassifier(max_depth=3, learning_rate=0.9, n_estimators=100, min_child_weight=0)\n",
    "\n",
    "print('Under-sampled model using XGB')\n",
    "\n",
    "probs = auto_ensemble(df_list, Xtea, ytea, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T03:21:13.091415Z",
     "start_time": "2019-03-07T03:21:12.504742Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ensemble of all Under-sampled data\n",
    "\n",
    "df_list = df_all_us\n",
    "\n",
    "# Tuned Model\n",
    "model = MultinomialNB(alpha=0.0000000001)\n",
    "\n",
    "print('Ensemble of all Under-sampled data')\n",
    "\n",
    "probs = auto_ensemble(df_list, Xtea, ytea, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T03:21:26.216618Z",
     "start_time": "2019-03-07T03:21:13.094413Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ensemble of all Under-sampled data using XGB\n",
    "# This takes 13 seconds or so to run\n",
    "\n",
    "df_list = df_all_us\n",
    "\n",
    "# Tuned Model\n",
    "model = XGBClassifier(max_depth=3, learning_rate=0.9, n_estimators=100, min_child_weight=0)\n",
    "\n",
    "print('Ensemble of all Under-sampled data')\n",
    "\n",
    "probs = auto_ensemble(df_list, Xtea, ytea, model = model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The under-sampled data sets do a much better job of predicting the minority class than the other models. Using XGBoost we can further increase the scores in a significant way. Clearly the better predictions of the minority class effect the prediction accuracy of the majority class. The overall accuracy is worse than the over-sampled sets but this model has the best predictions of the minority class.  \n",
    "  \n",
    "A combination of this ensemble of under sampled data using the XGBoost model with other ensembles may be the best method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T03:21:27.347928Z",
     "start_time": "2019-03-07T03:21:26.220616Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ensemble of all different data 1\n",
    "\n",
    "df_list = df_new_list1\n",
    "\n",
    "# Tuned Model\n",
    "model = MultinomialNB(alpha=0.0000000001)\n",
    "\n",
    "print('Ensemble of all different data 1')\n",
    "\n",
    "probs = auto_ensemble(df_list, Xtea, ytea, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T03:21:28.172418Z",
     "start_time": "2019-03-07T03:21:27.349925Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ensemble of all different data 2\n",
    "\n",
    "df_list = df_new_list2\n",
    "\n",
    "# Tuned Model\n",
    "model = MultinomialNB(alpha=0.0000000001)\n",
    "\n",
    "print('Ensemble of all different data 2')\n",
    "\n",
    "probs = auto_ensemble(df_list, Xtea, ytea, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T03:21:28.768064Z",
     "start_time": "2019-03-07T03:21:28.175417Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ensemble of all different data 3\n",
    "\n",
    "df_list = df_new_list3\n",
    "\n",
    "# Tuned Model\n",
    "model = MultinomialNB(alpha=0.0000000001)\n",
    "\n",
    "print('Ensemble of all different data 3')\n",
    "\n",
    "probs = auto_ensemble(df_list, Xtea, ytea, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T03:21:38.028574Z",
     "start_time": "2019-03-07T03:21:28.770064Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ensemble of all different data 3\n",
    "\n",
    "df_list = df_new_list3\n",
    "\n",
    "# Tuned Model\n",
    "model = XGBClassifier(max_depth=3, learning_rate=0.9, n_estimators=100, min_child_weight=0)\n",
    "\n",
    "print('Ensemble of all different data 3 with XGB')\n",
    "\n",
    "probs = auto_ensemble(df_list, Xtea, ytea, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T03:21:38.283422Z",
     "start_time": "2019-03-07T03:21:38.032572Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ensemble of all different data 4\n",
    "\n",
    "df_list = df_new_list4\n",
    "\n",
    "# Tuned Model\n",
    "model = MultinomialNB(alpha=0.0000000001)\n",
    "\n",
    "print('Ensemble of all different data 4')\n",
    "\n",
    "probs = auto_ensemble(df_list, Xtea, ytea, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T03:21:39.136903Z",
     "start_time": "2019-03-07T03:21:38.285421Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ensemble of all different data 5\n",
    "\n",
    "df_list = df_new_list5\n",
    "\n",
    "# Tuned Model\n",
    "model = MultinomialNB(alpha=0.0000000001)\n",
    "\n",
    "print('Ensemble of all different data 5')\n",
    "\n",
    "probs = auto_ensemble(df_list, Xtea, ytea, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T03:21:39.497681Z",
     "start_time": "2019-03-07T03:21:39.138902Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ensemble of all different data 6\n",
    "\n",
    "df_list = df_new_list6\n",
    "\n",
    "# Tuned Model\n",
    "model = MultinomialNB(alpha=0.0000000001)\n",
    "\n",
    "print('Ensemble of all different data 6')\n",
    "\n",
    "#probs = auto_ensemble(df_all_us, Xtea, ytea, model = model)\n",
    "probs = auto_ensemble(df_list, Xtea, ytea, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T03:21:40.214246Z",
     "start_time": "2019-03-07T03:21:39.500680Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ensemble of all different data 7\n",
    "\n",
    "df_list = df_new_list7\n",
    "\n",
    "# Tuned Model\n",
    "model = MultinomialNB(alpha=0.0000000001)\n",
    "\n",
    "print('Ensemble of all different data 7')\n",
    "\n",
    "#probs = auto_ensemble(df_all_us, Xtea, ytea, model = model)\n",
    "probs = auto_ensemble(df_list, Xtea, ytea, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T03:21:56.506855Z",
     "start_time": "2019-03-07T03:21:40.216246Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ensemble of all different data 7\n",
    "\n",
    "df_list = df_new_list7\n",
    "\n",
    "# Tuned Model\n",
    "model = XGBClassifier(max_depth=3, learning_rate=0.9, n_estimators=100, min_child_weight=0)\n",
    "\n",
    "print('Ensemble of all different data 7 with XGB')\n",
    "\n",
    "#probs = auto_ensemble(df_all_us, Xtea, ytea, model = model)\n",
    "probs = auto_ensemble(df_list, Xtea, ytea, model = model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best model?\n",
    "Oversampling using **SMOTE** and **XGB** seemed to give the best results, but only marginally over simply using a tuned **XGB** model None of the simple ensemble models seem to help the over all accuracy the most. However some did better model the minority class better so if that is the goal then one of those models should be picked. Although the **XGB** model on the all the data seemed to do almost as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the model to the unknown test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T00:21:01.355701Z",
     "start_time": "2019-03-08T00:21:01.341706Z"
    }
   },
   "outputs": [],
   "source": [
    "# need to take out the y_test and print scores from the functions.\n",
    "\n",
    "def automate_tf2_test(X_train, y_train, X_test, model, is_SMOTE = False):\n",
    "    ''' This function takes X_train, y_train, X_test, y_test, and the a tuned classification model then fits it and outputs\n",
    "    scores and a confusion matrix as well as the probabilities of the classification model.\n",
    "    '''\n",
    "\n",
    "    vectorizer = CountVectorizer() # Slightly better without stop words\n",
    "\n",
    "    # Fit and Transform the training data: X_train_t \n",
    "    X_train_t = vectorizer.fit_transform(X_train.values)\n",
    "\n",
    "    # Transform the test data: X_test_t \n",
    "    X_test_t = vectorizer.transform(X_test.values)\n",
    "        \n",
    "    if is_SMOTE == True:\n",
    "        sm = SMOTE(random_state=12, ratio = 1.0)\n",
    "        X_train_t, y_train = sm.fit_sample(X_train_t, y_train)\n",
    "    \n",
    "    # Fit the classifier to the training data\n",
    "    model.fit(X_train_t, y_train)\n",
    "    \n",
    "    pred_probs = model.predict_proba(X_test_t)\n",
    "\n",
    "    return (pred_probs)\n",
    "\n",
    "\n",
    "# Automate function to acount for ensembling comparisons\n",
    "\n",
    "def auto_ensemble_test(list_dfs, X_test, model, is_SMOTE = False):\n",
    "    ''' This function automates the ensemble process. It takes a list of dfs, models the data, and averages the results\n",
    "    of each data set then takes the average results and computes the scores and confusion matrix of the ensembled results. \n",
    "    inputs:\n",
    "    list_dfs is a list of data frames with both x and y columns.\n",
    "    X_test, y_test: are pandas series presumably from the hold out set.\n",
    "    model: model to be used in order of the list of dfs\n",
    "    '''\n",
    "    \n",
    "    pred_probs_tot = 0\n",
    "        \n",
    "    for df in list_dfs:\n",
    "        X_train = df['text']\n",
    "        y_train = df['flag']\n",
    "        pred_prob0 = automate_tf2_test(X_train, y_train, X_test, model, is_SMOTE)\n",
    "        pred_probs_tot = pred_probs_tot + np.array(pred_prob0)\n",
    "    \n",
    "    pred_probs_ave = pred_probs_tot[:,1]/len(list_dfs)\n",
    "    \n",
    "    pred_ave_int = pred_probs_ave.round(0).astype(int)\n",
    "\n",
    "    \n",
    "    return (pred_probs_ave, pred_ave_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T00:22:59.785843Z",
     "start_time": "2019-03-08T00:22:58.697981Z"
    }
   },
   "outputs": [],
   "source": [
    "# load in the test data\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T00:24:16.425680Z",
     "start_time": "2019-03-08T00:23:34.257187Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the model to the original data using SMOTE and XGB tuned\n",
    "\n",
    "df_list = [data]\n",
    "\n",
    "X_test1 = test['question_text']\n",
    "\n",
    "# XGB Model Tuned on the trained set using a holdout\n",
    "#model = XGBClassifier(max_depth=3, learning_rate=0.9, n_estimators=300, min_child_weight=0)\n",
    "model = MultinomialNB(alpha=0.0000000001)\n",
    "\n",
    "#print('Original Train data with SMOTE and XGB')\n",
    "print('Original Train data with NB model')\n",
    "\n",
    "# Out put the probabilites and flags for new data\n",
    "probs = auto_ensemble_test(df_list, X_test1, model = model, is_SMOTE = False)\n",
    "\n",
    "print(probs[0][16:40])\n",
    "print(probs[1][16:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T00:30:04.851701Z",
     "start_time": "2019-03-08T00:30:04.805730Z"
    }
   },
   "outputs": [],
   "source": [
    "# See the solution table!\n",
    "\n",
    "test['probs'] = probs[0]\n",
    "test['prediction'] = probs[1]\n",
    "\n",
    "solution = pd.concat([test['qid'], test['prediction']], axis=1)\n",
    "solution[12:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the final solution table as .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T00:30:08.784684Z",
     "start_time": "2019-03-08T00:30:07.360397Z"
    }
   },
   "outputs": [],
   "source": [
    "solution.to_csv('submision.csv', index=False) # save table to .csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the Model by Looking at the Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T03:22:21.816952Z",
     "start_time": "2019-03-07T03:22:21.788970Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the class labels: class_labels\n",
    "class_labels = nb_classifier_t.classes_\n",
    "\n",
    "# Extract the features: feature_names\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
    "feat_with_weights = sorted(zip(nb_classifier_t.coef_[0], feature_names))\n",
    "\n",
    "# Print the first class label and the top 30 feat_with_weights entries\n",
    "print(class_labels[0], feat_with_weights[:30])\n",
    "\n",
    "print('') # print a break\n",
    "\n",
    "# Print the second class label and the bottom 30 feat_with_weights entries\n",
    "print(class_labels[1], feat_with_weights[-30:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like the words most associated with preventative care are vaccines, boosters, year, annual, and exam. We can see that it makes sense fore these to be associated with preventative care and as such should not be included in non-preventative claims."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The rest of the notebook is just some code used to tinker with and tune the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunning and some different classifier algorithms: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T21:03:43.373818Z",
     "start_time": "2019-03-04T21:03:41.404493Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use a XGBoost method to model the TfidfVectorizer data\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Instantiate a XGBoost classifier: xgb_classifier_t tune with different parameters\n",
    "xgb_classifier_t = XGBClassifier(max_depth=3, learning_rate=0.9, n_estimators=50, min_child_weight=0) # \n",
    "\n",
    "# Fit the classifier to the training data\n",
    "xgb_classifier_t.fit(tfidf_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred_xgbt = xgb_classifier_t.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score_t = metrics.accuracy_score(y_test, pred_xgbt)\n",
    "print(score_t)\n",
    "print('F1:', metrics.f1_score(y_test, pred_xgbt))\n",
    "print('Prec:', metrics.precision_score(y_test, pred_xgbt))\n",
    "print('Rec:', metrics.recall_score(y_test, pred_xgbt))\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm_xgbt = metrics.confusion_matrix(y_test, pred_xgbt, labels=[0,1])\n",
    "print(cm_xgbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T21:04:23.232119Z",
     "start_time": "2019-03-04T21:04:20.672701Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use a XGBoost method to model the CountVectorizer data\n",
    "\n",
    "# Instantiate a XGBoost classifier: nb_classifier tune with different parameters\n",
    "xgb_classifier_c = XGBClassifier(max_depth=3, learning_rate=0.9, n_estimators=50, min_child_weight=0) # \n",
    "\n",
    "# Fit the classifier to the training data\n",
    "xgb_classifier_c.fit(count_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred_xgbc = xgb_classifier_c.predict(count_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score_c = metrics.accuracy_score(y_test, pred_xgbc)\n",
    "print(score_c)\n",
    "print('F1:', metrics.f1_score(y_test, pred_xgbc))\n",
    "print('Prec:', metrics.precision_score(y_test, pred_xgbc))\n",
    "print('Rec:', metrics.recall_score(y_test, pred_xgbc))\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm_xgbc = metrics.confusion_matrix(y_test, pred_xgbc, labels=[0,1])\n",
    "print(cm_xgbc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like XGBoost on the can do a bit better if tuned with the CountVectorizer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T00:34:38.588591Z",
     "start_time": "2019-03-03T00:32:26.403777Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tune the XGBoost method to model the TfidfVectorizer data\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "np.arange(0.000000001, 0.91, 0.1)\n",
    "param_grid = {'max_depth': np.arange(3, 4, 1), \n",
    "              'learning_rate': np.arange(0.9, 1, 0.1), \n",
    "              'n_estimators': np.arange(10, 20, 10), \n",
    "              'min_child_weight': np.arange(0, 1, 1)}\n",
    "\n",
    "# Instantiate a XGBoost classifier: nb_classifier tune with different parameters\n",
    "#xgb_classifier_c = XGBClassifier(max_depth=3, learning_rate=0.9, n_estimators=80, min_child_weight=0) # \n",
    "xgb_classifier_c = XGBClassifier()\n",
    "\n",
    "# Instantiate the GridSearchCV object: logreg_cv\n",
    "kf = KFold(5, shuffle = True, random_state = 42) # creates the kfold object with random shuffling\n",
    "grid_cv_xgb = GridSearchCV(xgb_classifier_c, param_grid, cv = kf)\n",
    "\n",
    "# Fit it to the training data\n",
    "grid_cv_xgb.fit(tfidf_train, y_train)\n",
    "\n",
    "# Print the optimal parameters and best score\n",
    "print(\"Best alpha: {}\".format(grid_cv_xgb.best_params_))\n",
    "print(\"Best Accuracy: {}\".format(grid_cv_xgb.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T00:52:57.831959Z",
     "start_time": "2019-03-03T00:52:57.422897Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use a Naive Bayes method to model the TfidfVectorizer data\n",
    "\n",
    "# Fit and Transform the training data: tfidf_X \n",
    "tfidf_X = tfidf_vectorizer.fit_transform(X.values)\n",
    "\n",
    "# Import necessary modules\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier tune with different alphas\n",
    "nb_classifier_t = MultinomialNB(alpha=0.1) # alpha 0.1 seems to give the best results and beats CountVectorizer best results\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier_t.fit(tfidf_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred_prob_t = nb_classifier_t.predict_proba(tfidf_test)[:,1]\n",
    "#pred_t = nb_classifier_t.predict(tfidf_test)\n",
    "\n",
    "# Compute and print AUC score\n",
    "print(\"AUC: {}\".format(roc_auc_score(y_test, pred_prob_t)))\n",
    "\n",
    "# Compute cross-validated AUC scores: cv_auc\n",
    "kf = KFold(5, shuffle = True, random_state = 42) # creates the kfold object with random shuffling\n",
    "cv_auc = cross_val_score(nb_classifier_t, tfidf_X, y, cv = kf, scoring = 'roc_auc')\n",
    "\n",
    "# Print list of AUC scores\n",
    "print(\"AUC scores computed using 5-fold cross-validation: {}\".format(cv_auc))\n",
    "print(\"Average AUC scores computed using 5-fold cross-validation: {}\".format(cv_auc.mean().round(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T00:44:51.402689Z",
     "start_time": "2019-03-03T00:44:50.656135Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'alpha': np.arange(0.000000001, 0.91, 0.1)}\n",
    "\n",
    "nb_classifier_n = MultinomialNB()\n",
    "\n",
    "# Instantiate the GridSearchCV object: logreg_cv\n",
    "kf = KFold(5, shuffle = True, random_state = 42) # creates the kfold object with random shuffling\n",
    "grid_cv = GridSearchCV(nb_classifier_n, param_grid, cv = kf)\n",
    "\n",
    "# Fit it to the training data\n",
    "grid_cv.fit(tfidf_train, y_train)\n",
    "\n",
    "# Print the optimal parameters and best score\n",
    "print(\"Best alpha: {}\".format(grid_cv.best_params_))\n",
    "print(\"Best Accuracy: {}\".format(grid_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T23:36:26.412630Z",
     "start_time": "2019-03-06T23:36:26.367661Z"
    }
   },
   "outputs": [],
   "source": [
    "solution.to_csv('p2_solution.csv') # save table to .csv file\n",
    "solution.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
